{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import math\n",
    "import scipy.stats as st\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(\"../..\")))  # if you're in optimizer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../simulated_log.csv')\n",
    "raw = pd.read_csv('../../raw_data/LoanApp.csv.gz', compression='gzip')\n",
    "\n",
    "\n",
    "with open(\"../parameters/simulation_parameters.pkl\", \"rb\") as f:\n",
    "    sim_params = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_dict = sim_params[\"transition_probabilities\"]\n",
    "durations = sim_params[\"activity_durations_dict\"]\n",
    "calendars = sim_params[\"res_calendars\"]  # {agent_id: RCalendar}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation for traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_distribution(distribution):\n",
    "    \"\"\"\n",
    "    Copy of the AgentSimulator utils method\n",
    "    \"\"\"\n",
    "    if distribution.type.value == \"expon\":\n",
    "        scale = distribution.mean - distribution.min\n",
    "        if scale < 0.0:\n",
    "            print(\"Warning! Trying to generate EXPON sample with 'mean' < 'min', using 'mean' as scale value.\")\n",
    "            scale = distribution.mean\n",
    "        sample = st.expon.rvs(loc=distribution.min, scale=scale, size=1)\n",
    "    elif distribution.type.value == \"gamma\":\n",
    "        # If the distribution corresponds to a 'gamma' with loc!=0, the estimation is done wrong\n",
    "        # dunno how to take that into account\n",
    "        sample = st.gamma.rvs(\n",
    "            pow(distribution.mean, 2) / distribution.var,\n",
    "            loc=0,\n",
    "            scale=distribution.var / distribution.mean,\n",
    "            size=1,\n",
    "        )\n",
    "    elif distribution.type.value == \"norm\":\n",
    "        sample = st.norm.rvs(loc=distribution.mean, scale=distribution.std, size=1)\n",
    "    elif distribution.type.value == \"uniform\":\n",
    "        sample = st.uniform.rvs(loc=distribution.min, scale=distribution.max - distribution.min, size=1)\n",
    "    elif distribution.type.value == \"lognorm\":\n",
    "        # If the distribution corresponds to a 'lognorm' with loc!=0, the estimation is done wrong\n",
    "        # dunno how to take that into account\n",
    "        pow_mean = pow(distribution.mean, 2)\n",
    "        phi = math.sqrt(distribution.var + pow_mean)\n",
    "        mu = math.log(pow_mean / phi)\n",
    "        sigma = math.sqrt(math.log(phi ** 2 / pow_mean))\n",
    "        sample = st.lognorm.rvs(sigma, loc=0, scale=math.exp(mu), size=1)\n",
    "    elif distribution.type.value == \"fix\":\n",
    "        sample = [distribution.mean] * 1\n",
    "\n",
    "    return sample[0]\n",
    "\n",
    "def remove_transitive_response_constraints(response_constraints):\n",
    "    cleaned = {}\n",
    "\n",
    "    for act, responses in response_constraints.items():\n",
    "        direct = set(responses)\n",
    "\n",
    "        # Remove any activity that is indirectly reachable through other responses\n",
    "        for mid in responses:\n",
    "            indirects = set(response_constraints.get(mid, []))\n",
    "            direct -= indirects\n",
    "\n",
    "        cleaned[act] = list(direct)\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def discover_post_conditions(df, activity_col='activity', case_col='case_id', order_by='end_time'):\n",
    "    response_counts = defaultdict(lambda: defaultdict(int))\n",
    "    activity_counts = defaultdict(int)\n",
    "\n",
    "    # Group by case\n",
    "    for case_id, group in df.groupby(case_col):\n",
    "        sorted_activities = group.sort_values(by=order_by)[activity_col].tolist()\n",
    "\n",
    "        for i, act in enumerate(sorted_activities):\n",
    "            activity_counts[act] += 1\n",
    "\n",
    "            # All activities that come after this one\n",
    "            for after_act in sorted_activities[i+1:]:\n",
    "                response_counts[act][after_act] += 1\n",
    "\n",
    "    # Build final response constraint map\n",
    "    post_conditions = {}\n",
    "    for act, after_acts in response_counts.items():\n",
    "        constraints = []\n",
    "        for after_act, count in after_acts.items():\n",
    "            # Threshold: e.g., B happens after A in 80%+ of A occurrences\n",
    "            if count / activity_counts[act] >= 0.8:\n",
    "                constraints.append(after_act)\n",
    "        if constraints:\n",
    "            post_conditions[act] = constraints\n",
    "\n",
    "    post_conditions = remove_transitive_response_constraints(post_conditions)\n",
    "\n",
    "    return post_conditions\n",
    "\n",
    "\n",
    "def extract_all_successors(transition_dict):\n",
    "    \"\"\"\n",
    "    Converts a nested transition dictionary to a flat mapping:\n",
    "    prefix_activity → list of all possible successor activities (non-zero prob)\n",
    "\n",
    "    Parameters:\n",
    "    - transition_dict: dict of {prefix: {agent_id: {activity: prob}}} ! Careful, only one activity prefixes are valid\n",
    "\n",
    "    Returns:\n",
    "    - dict of {activity: [possible next activities]}\n",
    "    \"\"\"\n",
    "    flat_successors = {}\n",
    "    seen_anchors = set()\n",
    "\n",
    "    for prefix, agent_dict in transition_dict.items():\n",
    "        if not prefix:\n",
    "            continue  # skip empty prefixes\n",
    "        if prefix[-1] in seen_anchors:\n",
    "            continue\n",
    "        anchor = prefix[-1]  # last activity in the prefix\n",
    "        seen_anchors.add(anchor)\n",
    "\n",
    "        successor_set = set()\n",
    "        for agent_transitions in agent_dict.values():\n",
    "            for act, prob in agent_transitions.items():\n",
    "                if prob > 0:\n",
    "                    successor_set.add(act)\n",
    "\n",
    "        flat_successors[anchor] = sorted(successor_set)\n",
    "\n",
    "    return flat_successors\n",
    "\n",
    "\n",
    "def mine_concurrent_activities(df, case_col='case_id', activity_col='activity',\n",
    "                                start_col='start_time', end_col='end_time'):\n",
    "    \"\"\"\n",
    "    For each activity, detect other activities that can run concurrently\n",
    "    by overlapping time windows in the same case.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Event log with case_id, activity, start_time, end_time\n",
    "\n",
    "    Returns:\n",
    "    - co_occurrence: dict {activity: [other activities that overlapped with it]}\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[start_col] = pd.to_datetime(df[start_col], format='mixed', utc=True)\n",
    "    df[end_col] = pd.to_datetime(df[end_col], format='mixed', utc=True)\n",
    "\n",
    "\n",
    "    co_occurrence = defaultdict(set)\n",
    "\n",
    "    for case_id, group in df.groupby(case_col):\n",
    "        group = group.sort_values(by=start_col)\n",
    "        for i, row_i in group.iterrows():\n",
    "            act_i, start_i, end_i = row_i[activity_col], row_i[start_col], row_i[end_col]\n",
    "            for j, row_j in group.iterrows():\n",
    "                if i == j:\n",
    "                    continue\n",
    "                act_j, start_j, end_j = row_j[activity_col], row_j[start_col], row_j[end_col]\n",
    "                # Check for overlap\n",
    "                if start_i < end_j and start_j < end_i:\n",
    "                    co_occurrence[act_i].add(act_j)\n",
    "\n",
    "    # Convert sets to sorted lists\n",
    "    return {act: sorted(list(others)) for act, others in co_occurrence.items()}\n",
    "\n",
    "def extract_xor_groups_from_cooccurrence(successor_map, co_occurrence_map):\n",
    "    \"\"\"\n",
    "    Builds XOR groups from a simplified co-occurrence map.\n",
    "    An activity is excluded from XOR groups if it can co-occur with the anchor.\n",
    "    Returns:\n",
    "    - xor_groups: {anchor: list of mutually exclusive groups (each group is a list of activities)}\n",
    "    \"\"\"\n",
    "    xor_groups = defaultdict(list)\n",
    "\n",
    "    for anchor, successors in successor_map.items():\n",
    "        if not successors:\n",
    "            continue\n",
    "\n",
    "        # ⚠️ Filter out successors that can co-occur with the anchor\n",
    "        filtered_successors = [\n",
    "            act for act in successors\n",
    "            if act not in co_occurrence_map.get(anchor, []) and anchor not in co_occurrence_map.get(act, [])\n",
    "        ]\n",
    "\n",
    "        remaining = set(filtered_successors)\n",
    "        groups = []\n",
    "\n",
    "        while remaining:\n",
    "            act = remaining.pop()\n",
    "            group = {act}\n",
    "\n",
    "            for other in list(remaining):\n",
    "                if (\n",
    "                    act in co_occurrence_map.get(other, []) or\n",
    "                    other in co_occurrence_map.get(act, [])\n",
    "                ):\n",
    "                    group.add(other)\n",
    "                    remaining.remove(other)\n",
    "\n",
    "            groups.append(sorted(group))\n",
    "\n",
    "        if len(groups) > 1:\n",
    "            xor_groups[anchor] = groups\n",
    "\n",
    "    return xor_groups\n",
    "\n",
    "# Note to self. This does not cover cases like: Reject application cannot happen after Approve application\n",
    "\n",
    "def discover_prerequisites_from_log(df, activity_col='activity', case_col='case_id', order_by='end_time'):\n",
    "    # Step 1: Collect all activities that appear before each activity in each case\n",
    "    activity_to_preceding_sets = defaultdict(list)\n",
    "\n",
    "    for case_id, group in df.groupby(case_col):\n",
    "        sorted_activities = group.sort_values(by=order_by)[activity_col].tolist()\n",
    "        seen = set()\n",
    "        for i, act in enumerate(sorted_activities):\n",
    "            activity_to_preceding_sets[act].append(seen.copy())\n",
    "            seen.add(act)\n",
    "\n",
    "    # Step 2: Intersect the \"seen-before\" sets across all cases\n",
    "    raw_prerequisites = {}\n",
    "    for act, preceding_sets in activity_to_preceding_sets.items():\n",
    "        if preceding_sets:\n",
    "            raw_prerequisites[act] = set.intersection(*preceding_sets)\n",
    "        else:\n",
    "            raw_prerequisites[act] = set()\n",
    "\n",
    "    # Step 3: Remove transitive dependencies\n",
    "    # If A → B and B → C, remove A from prerequisites of C\n",
    "    def remove_transitive(prereq_dict):\n",
    "        cleaned = {}\n",
    "        for act in prereq_dict:\n",
    "            direct_prereqs = prereq_dict[act].copy()\n",
    "            # Remove any indirect dependencies\n",
    "            for p in direct_prereqs.copy():\n",
    "                indirects = prereq_dict.get(p, set())\n",
    "                direct_prereqs -= indirects\n",
    "            cleaned[act] = list(direct_prereqs)\n",
    "        return cleaned\n",
    "\n",
    "    strict_prerequisites = remove_transitive(raw_prerequisites)\n",
    "    return strict_prerequisites\n",
    "\n",
    "\n",
    "\n",
    "def validate_simulated_log(df, prerequisites, post_conditions, valid_end_activities, \n",
    "                            xor_rules=None, case_col='case_id', activity_col='activity', order_by='start'):\n",
    "    issues = []\n",
    "\n",
    "    for case_id, group in df.groupby(case_col):\n",
    "        sorted_activities = group.sort_values(by=order_by)[activity_col].tolist()\n",
    "\n",
    "        if not sorted_activities:\n",
    "            issues.append((case_id, \"Empty trace\"))\n",
    "            continue\n",
    "\n",
    "        activities_no_end = [a for a in sorted_activities if a != \"zzz_end\"]\n",
    "\n",
    "        # 🚨 1. Prerequisites check\n",
    "        performed = set()\n",
    "        for act in activities_no_end:\n",
    "            required = prerequisites.get(act, [])\n",
    "            if not all(pre in performed for pre in required):\n",
    "                missing = [pre for pre in required if pre not in performed]\n",
    "                issues.append((case_id, f\"Activity '{act}' missing prerequisites {missing}\"))\n",
    "            performed.add(act)\n",
    "\n",
    "        # 🚨 2. Post-conditions check\n",
    "        for i, act in enumerate(activities_no_end):\n",
    "            required_posts = post_conditions.get(act, [])\n",
    "            future_acts = set(activities_no_end[i+1:])\n",
    "            for post in required_posts:\n",
    "                if post not in future_acts:\n",
    "                    issues.append((case_id, f\"Activity '{act}' missing required post-condition '{post}'\"))\n",
    "\n",
    "        # 🚨 3. End correctness check\n",
    "        if activities_no_end:\n",
    "            last_real_activity = activities_no_end[-1]\n",
    "            if last_real_activity not in valid_end_activities:\n",
    "                issues.append((case_id, f\"Case ends incorrectly on '{last_real_activity}'\"))\n",
    "\n",
    "        # 🚨 4. XOR violation check\n",
    "        if xor_rules:\n",
    "            for anchor, groups in xor_rules.items():\n",
    "                current_window = set()\n",
    "                windows = []\n",
    "\n",
    "                for act in activities_no_end:\n",
    "                    if act == anchor:\n",
    "                        # Anchor re-executed → start new window\n",
    "                        if current_window:\n",
    "                            windows.append(current_window)\n",
    "                        current_window = set()\n",
    "                    else:\n",
    "                        for idx, group in enumerate(groups):\n",
    "                            if act in group:\n",
    "                                current_window.add(idx)\n",
    "\n",
    "                # Add final window\n",
    "                if current_window:\n",
    "                    windows.append(current_window)\n",
    "\n",
    "                # Now validate all XOR windows\n",
    "                for win_idx, window in enumerate(windows):\n",
    "                    if len(window) > 1:\n",
    "                        issues.append((case_id, f\"XOR violation for anchor '{anchor}' in window {win_idx}: multiple groups executed {list(window)}\"))\n",
    "\n",
    "    return issues\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_agents(agents, durations, calendars=None):\n",
    "    issues = []\n",
    "\n",
    "    for agent in agents:\n",
    "        agent_id = agent.agent_id\n",
    "\n",
    "        # 1. Check capabilities\n",
    "        if not agent.capable_activities:\n",
    "            issues.append((agent_id, \"Agent has no capable activities\"))\n",
    "\n",
    "        for act in agent.capable_activities:\n",
    "            dur = durations.get(agent_id, {}).get(act)\n",
    "            if not dur or isinstance(dur, list):\n",
    "                issues.append((agent_id, f\"Activity '{act}' has no valid duration distribution\"))\n",
    "\n",
    "        # 2. Check calendar coverage\n",
    "        if calendars:\n",
    "            calendar = calendars.get(agent_id)\n",
    "            if calendar is None:\n",
    "                issues.append((agent_id, \"Missing calendar\"))\n",
    "            elif not calendar.to_dict().get('time_periods', None):\n",
    "                issues.append((agent_id, \"Calendar has no time periods\"))\n",
    "\n",
    "\n",
    "    return issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentStub: \n",
    "    def __init__(self, agent_id, capable_activities, calendar):\n",
    "        self.agent_id = agent_id\n",
    "        self.capable_activities = capable_activities\n",
    "        self.calendar = calendar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_agent_calendars(calendars, agent_ids=None, max_agents=10):\n",
    "    \"\"\"\n",
    "    Pretty print the calendar time periods for each agent.\n",
    "    \n",
    "    Parameters:\n",
    "    - calendars: dict of {agent_id: RCalendar}\n",
    "    - agent_ids: optional list of agent IDs to filter (default: all)\n",
    "    - max_agents: limit number of agents printed (default: 10)\n",
    "    \"\"\"\n",
    "    if agent_ids is None:\n",
    "        agent_ids = list(calendars.keys())\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for agent_id in agent_ids:\n",
    "        calendar = calendars.get(agent_id)\n",
    "        if calendar is None:\n",
    "            print(f\"Agent {agent_id}: ❌ No calendar assigned\")\n",
    "            continue\n",
    "\n",
    "        # Try accessing time_periods safely\n",
    "        time_periods = calendar.to_dict().get('time_periods', None)\n",
    "        if not time_periods:\n",
    "            print(f\"Agent {agent_id}: ⚠️ Calendar exists but has no time periods\")\n",
    "        else:\n",
    "            print(f\"\\nAgent {agent_id} ✅ Calendar time periods:\")\n",
    "            for period in time_periods:\n",
    "                print(f\"  - {period['from']} → {period['to']}, {period['beginTime']}–{period['endTime']}\")\n",
    "\n",
    "        count += 1\n",
    "        if count >= max_agents:\n",
    "            print(f\"\\n...and {len(agent_ids) - max_agents} more. Use `max_agents=None` to show all.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation of my simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Validation passed with no issues!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Discover prerequisites and post-conditions\n",
    "prerequisites = discover_prerequisites_from_log(raw, activity_col='activity', case_col='case_id', order_by='end_time')\n",
    "post_conditions = discover_post_conditions(raw, activity_col='activity', case_col='case_id', order_by='end_time')\n",
    "\n",
    "# Step 2: Determine valid end activities (from transition dict if you have it)\n",
    "# If you don't have a transition dict handy, you can approximate:\n",
    "valid_end_activities = raw.groupby('case_id')['activity'].last().value_counts().index.tolist()\n",
    "\n",
    "# Step 3: Extract successor and concurrency maps\n",
    "successor_map = extract_all_successors(transition_dict)  # <- you must define/provide transition_dict\n",
    "co_occurrence_map = mine_concurrent_activities(raw, case_col='case_id', activity_col='activity',\n",
    "                                               start_col='start_time', end_col='end_time')\n",
    "\n",
    "# Step 4: Infer XOR rules\n",
    "xor_rules = extract_xor_groups_from_cooccurrence(successor_map, co_occurrence_map)\n",
    "\n",
    "# Step 5: Validate log\n",
    "issues = validate_simulated_log(df,\n",
    "                                prerequisites=prerequisites,\n",
    "                                post_conditions=post_conditions,\n",
    "                                valid_end_activities=valid_end_activities,\n",
    "                                xor_rules=xor_rules,\n",
    "                                case_col='case_id',\n",
    "                                activity_col='activity',\n",
    "                                order_by='start')\n",
    "\n",
    "# Step 6: Review results\n",
    "if issues:\n",
    "    print(f\"⚠️ Validation found {len(issues)} issues:\")\n",
    "    for case_id, desc in issues:\n",
    "        print(f\"  - Case {case_id}: {desc}\")\n",
    "else:\n",
    "    print(\"✅ Validation passed with no issues!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All agents are valid!\n"
     ]
    }
   ],
   "source": [
    "agents = []\n",
    "\n",
    "for agent_id, acts in durations.items():\n",
    "    # Only include activities with a valid duration distribution\n",
    "    capable_activities = {\n",
    "        act for act, dist in acts.items()\n",
    "        if dist and not isinstance(dist, list)\n",
    "    }\n",
    "    # print(f\"Agent{agent_id} can do {capable_activities}\")\n",
    "\n",
    "    calendar = calendars.get(agent_id)\n",
    "\n",
    "    agents.append(AgentStub(agent_id, capable_activities, calendar))\n",
    "\n",
    "agent_issues = validate_agents(agents, durations, calendars)\n",
    "\n",
    "if agent_issues:\n",
    "    print(f\"⚠️ Found {len(agent_issues)} agent issues:\")\n",
    "    for agent_id, msg in agent_issues:\n",
    "        print(f\"  - Agent {agent_id}: {msg}\")\n",
    "else:\n",
    "    print(\"✅ All agents are valid!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation of AgentSimulator Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_simulated_logs(base_simulation_dir):\n",
    "    \"\"\"\n",
    "    Load all simulated log CSVs from a given base directory into a single DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "        base_simulation_dir (str): Path to the directory containing simulated_log_*.csv files.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A combined DataFrame of all simulated logs, with a 'simulation_run' column.\n",
    "    \"\"\"\n",
    "    simulated_logs = []\n",
    "    for root, dirs, files in os.walk(base_simulation_dir):\n",
    "        for file in files:\n",
    "            if file.startswith(\"simulated_log_\") and file.endswith(\".csv\"):\n",
    "                simulated_logs.append(os.path.join(root, file))\n",
    "\n",
    "    simulated_log_dfs = []\n",
    "    for log_path in simulated_logs:\n",
    "        df = pd.read_csv(log_path)\n",
    "        simulation_index = int(os.path.basename(log_path).split('_')[-1].split('.')[0])\n",
    "        df[\"simulation_run\"] = simulation_index\n",
    "        simulated_log_dfs.append(df)\n",
    "\n",
    "    if simulated_log_dfs:\n",
    "        return pd.concat(simulated_log_dfs, ignore_index=True)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"No simulated logs found in {base_simulation_dir}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulated_logs_df = load_all_simulated_logs(\"../../simulated_data/LoanApp.csv/main_results/\")\n",
    "simulated_logs_df = simulated_logs_df.rename(columns={'activity_name': 'activity'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============================================\n",
      "SIMULATION 0\n",
      "✅ Validation passed with no issues!\n",
      "=============================================\n",
      "SIMULATION 1\n",
      "✅ Validation passed with no issues!\n",
      "=============================================\n",
      "SIMULATION 2\n",
      "✅ Validation passed with no issues!\n",
      "=============================================\n",
      "SIMULATION 3\n",
      "✅ Validation passed with no issues!\n",
      "=============================================\n",
      "SIMULATION 4\n",
      "⚠️ Validation found 10 issues:\n",
      "  - Case 5: Activity 'Assess loan risk' missing prerequisites ['AML check', 'Appraise property']\n",
      "  - Case 5: Activity 'Check application form completeness' missing required post-condition 'Appraise property'\n",
      "  - Case 5: Activity 'Check application form completeness' missing required post-condition 'AML check'\n",
      "  - Case 5: Activity 'Check application form completeness' missing required post-condition 'Appraise property'\n",
      "  - Case 5: Activity 'Check application form completeness' missing required post-condition 'AML check'\n",
      "  - Case 5: Activity 'Check application form completeness' missing required post-condition 'Appraise property'\n",
      "  - Case 5: Activity 'Check application form completeness' missing required post-condition 'AML check'\n",
      "  - Case 5: Activity 'Check application form completeness' missing required post-condition 'Appraise property'\n",
      "  - Case 5: Activity 'Check application form completeness' missing required post-condition 'AML check'\n",
      "  - Case 5: Activity 'Applicant completes form' missing required post-condition 'Check application form completeness'\n",
      "=============================================\n",
      "SIMULATION 5\n",
      "✅ Validation passed with no issues!\n",
      "=============================================\n",
      "SIMULATION 6\n",
      "✅ Validation passed with no issues!\n",
      "=============================================\n",
      "SIMULATION 7\n",
      "✅ Validation passed with no issues!\n",
      "=============================================\n",
      "SIMULATION 8\n",
      "⚠️ Validation found 6 issues:\n",
      "  - Case 152: Activity 'Assess loan risk' missing prerequisites ['Appraise property']\n",
      "  - Case 152: Activity 'Check application form completeness' missing required post-condition 'Appraise property'\n",
      "  - Case 152: Activity 'Check application form completeness' missing required post-condition 'Appraise property'\n",
      "  - Case 152: Activity 'Check application form completeness' missing required post-condition 'Appraise property'\n",
      "  - Case 152: Activity 'Check application form completeness' missing required post-condition 'Appraise property'\n",
      "  - Case 152: Activity 'Applicant completes form' missing required post-condition 'Check application form completeness'\n",
      "=============================================\n",
      "SIMULATION 9\n",
      "✅ Validation passed with no issues!\n"
     ]
    }
   ],
   "source": [
    "for run_id, run_df in simulated_logs_df.groupby('simulation_run'):\n",
    "    issues = validate_simulated_log(run_df,\n",
    "                                prerequisites=prerequisites,\n",
    "                                post_conditions=post_conditions,\n",
    "                                valid_end_activities=valid_end_activities,\n",
    "                                xor_rules=xor_rules,\n",
    "                                case_col='case_id',\n",
    "                                activity_col='activity',\n",
    "                                order_by='start_timestamp')\n",
    "    print(f\"=============================================\")\n",
    "    print(f\"SIMULATION {run_id}\")\n",
    "    if issues:\n",
    "        print(f\"⚠️ Validation found {len(issues)} issues:\")\n",
    "        for case_id, desc in issues:\n",
    "            print(f\"  - Case {case_id}: {desc}\")\n",
    "    else:\n",
    "        print(\"✅ Validation passed with no issues!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: The non-deterministic approach is sometimes not respecting pre-requisites and post-conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of CT and Activities per Trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison on CT per case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation of AS CT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "simulated_logs_df['start'] = pd.to_datetime(simulated_logs_df['start_timestamp'], utc=True, format='mixed')\n",
    "simulated_logs_df['end'] = pd.to_datetime(simulated_logs_df['end_timestamp'], utc=True, format='mixed')\n",
    "\n",
    "\n",
    "agent_sim_ct = simulated_logs_df.groupby(['simulation_run', 'case_id']).agg({\n",
    "    'start': 'min',\n",
    "    'end': 'max'\n",
    "}).reset_index()\n",
    "agent_sim_ct['cycle_time'] = (agent_sim_ct['end'] - agent_sim_ct['start']).dt.total_seconds()\n",
    "agent_sim_ct_stats = agent_sim_ct.groupby('case_id')['cycle_time'].agg(['mean', 'median', 'min', 'max']).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10393.653982</td>\n",
       "      <td>10461.180014</td>\n",
       "      <td>6272.208021</td>\n",
       "      <td>15001.063149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10279.448314</td>\n",
       "      <td>8976.440958</td>\n",
       "      <td>7217.301555</td>\n",
       "      <td>18802.096051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>8962.074078</td>\n",
       "      <td>9078.397932</td>\n",
       "      <td>6486.829351</td>\n",
       "      <td>12085.923660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10658.837560</td>\n",
       "      <td>9801.521694</td>\n",
       "      <td>5328.655841</td>\n",
       "      <td>16367.334062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6696.719043</td>\n",
       "      <td>6866.887639</td>\n",
       "      <td>3494.978034</td>\n",
       "      <td>9268.303847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>195</td>\n",
       "      <td>10102.579353</td>\n",
       "      <td>9669.424490</td>\n",
       "      <td>5217.406785</td>\n",
       "      <td>19050.544967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>196</td>\n",
       "      <td>16894.126598</td>\n",
       "      <td>10786.804415</td>\n",
       "      <td>6990.580330</td>\n",
       "      <td>78041.970868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>197</td>\n",
       "      <td>24745.176989</td>\n",
       "      <td>12715.299090</td>\n",
       "      <td>8529.066414</td>\n",
       "      <td>78388.558185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>198</td>\n",
       "      <td>8907.860860</td>\n",
       "      <td>7586.528884</td>\n",
       "      <td>6510.086878</td>\n",
       "      <td>14394.398955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>199</td>\n",
       "      <td>7797.832781</td>\n",
       "      <td>7728.018148</td>\n",
       "      <td>4735.515469</td>\n",
       "      <td>10795.646703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     case_id          mean        median          min           max\n",
       "0          0  10393.653982  10461.180014  6272.208021  15001.063149\n",
       "1          1  10279.448314   8976.440958  7217.301555  18802.096051\n",
       "2          2   8962.074078   9078.397932  6486.829351  12085.923660\n",
       "3          3  10658.837560   9801.521694  5328.655841  16367.334062\n",
       "4          4   6696.719043   6866.887639  3494.978034   9268.303847\n",
       "..       ...           ...           ...          ...           ...\n",
       "195      195  10102.579353   9669.424490  5217.406785  19050.544967\n",
       "196      196  16894.126598  10786.804415  6990.580330  78041.970868\n",
       "197      197  24745.176989  12715.299090  8529.066414  78388.558185\n",
       "198      198   8907.860860   7586.528884  6510.086878  14394.398955\n",
       "199      199   7797.832781   7728.018148  4735.515469  10795.646703\n",
       "\n",
       "[200 rows x 5 columns]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_sim_ct_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation of optimizer CT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['start'] = pd.to_datetime(df['start'], utc=True, format='mixed')\n",
    "df['end'] = pd.to_datetime(df['end'], utc=True, format='mixed')\n",
    "\n",
    "my_ct = df.groupby('case_id').agg({\n",
    "    'start': 'min',\n",
    "    'end': 'max'\n",
    "}).reset_index()\n",
    "my_ct['cycle_time'] = (my_ct['end'] - my_ct['start']).dt.total_seconds()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculation or raw CT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw['start'] = pd.to_datetime(raw['start_time'], utc=True, format='mixed')\n",
    "raw['end'] = pd.to_datetime(raw['end_time'], utc=True, format='mixed')\n",
    "\n",
    "my_raw_ct = raw.groupby('case_id').agg({\n",
    "    'start': 'min',\n",
    "    'end': 'max'\n",
    "}).reset_index()\n",
    "my_raw_ct['cycle_time'] = (my_raw_ct['end'] - my_raw_ct['start']).dt.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.merge(my_ct[['case_id', 'cycle_time']], agent_sim_ct_stats, on='case_id', suffixes=('_mine', '_agent'))\n",
    "comparison_df['Diff Median with Optimizer'] = comparison_df['median'] - comparison_df['cycle_time']\n",
    "comparison_df = comparison_df.rename({'cycle_time':'optimizer_CT'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.merge(comparison_df, my_raw_ct[['case_id', 'cycle_time']], on='case_id', suffixes=('_mine', '_raw'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>cycle_time_mine</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>Diff Median with Optimizer</th>\n",
       "      <th>cycle_time_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6176.217162</td>\n",
       "      <td>10393.653982</td>\n",
       "      <td>10461.180014</td>\n",
       "      <td>6272.208021</td>\n",
       "      <td>15001.063149</td>\n",
       "      <td>4284.962852</td>\n",
       "      <td>8805.244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13474.842794</td>\n",
       "      <td>10279.448314</td>\n",
       "      <td>8976.440958</td>\n",
       "      <td>7217.301555</td>\n",
       "      <td>18802.096051</td>\n",
       "      <td>-4498.401837</td>\n",
       "      <td>9431.527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>9578.152907</td>\n",
       "      <td>8962.074078</td>\n",
       "      <td>9078.397932</td>\n",
       "      <td>6486.829351</td>\n",
       "      <td>12085.923660</td>\n",
       "      <td>-499.754975</td>\n",
       "      <td>14622.587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7757.671680</td>\n",
       "      <td>10658.837560</td>\n",
       "      <td>9801.521694</td>\n",
       "      <td>5328.655841</td>\n",
       "      <td>16367.334062</td>\n",
       "      <td>2043.850014</td>\n",
       "      <td>6757.300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>13463.884679</td>\n",
       "      <td>6696.719043</td>\n",
       "      <td>6866.887639</td>\n",
       "      <td>3494.978034</td>\n",
       "      <td>9268.303847</td>\n",
       "      <td>-6596.997041</td>\n",
       "      <td>12222.587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>191</td>\n",
       "      <td>16532.897115</td>\n",
       "      <td>9503.117949</td>\n",
       "      <td>9610.302175</td>\n",
       "      <td>4223.323197</td>\n",
       "      <td>18675.799977</td>\n",
       "      <td>-6922.594940</td>\n",
       "      <td>12139.685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>192</td>\n",
       "      <td>12006.905836</td>\n",
       "      <td>9273.915904</td>\n",
       "      <td>8483.839910</td>\n",
       "      <td>4555.781406</td>\n",
       "      <td>18984.668347</td>\n",
       "      <td>-3523.065926</td>\n",
       "      <td>9667.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>194</td>\n",
       "      <td>15635.980532</td>\n",
       "      <td>10738.295483</td>\n",
       "      <td>8994.248712</td>\n",
       "      <td>6457.453908</td>\n",
       "      <td>19601.174884</td>\n",
       "      <td>-6641.731819</td>\n",
       "      <td>14403.291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>195</td>\n",
       "      <td>19091.963238</td>\n",
       "      <td>10102.579353</td>\n",
       "      <td>9669.424490</td>\n",
       "      <td>5217.406785</td>\n",
       "      <td>19050.544967</td>\n",
       "      <td>-9422.538748</td>\n",
       "      <td>5496.187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>198</td>\n",
       "      <td>8642.316668</td>\n",
       "      <td>8907.860860</td>\n",
       "      <td>7586.528884</td>\n",
       "      <td>6510.086878</td>\n",
       "      <td>14394.398955</td>\n",
       "      <td>-1055.787784</td>\n",
       "      <td>7550.780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>79 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    case_id  cycle_time_mine          mean        median          min  \\\n",
       "0         0      6176.217162  10393.653982  10461.180014  6272.208021   \n",
       "1         1     13474.842794  10279.448314   8976.440958  7217.301555   \n",
       "2         2      9578.152907   8962.074078   9078.397932  6486.829351   \n",
       "3         3      7757.671680  10658.837560   9801.521694  5328.655841   \n",
       "4         4     13463.884679   6696.719043   6866.887639  3494.978034   \n",
       "..      ...              ...           ...           ...          ...   \n",
       "74      191     16532.897115   9503.117949   9610.302175  4223.323197   \n",
       "75      192     12006.905836   9273.915904   8483.839910  4555.781406   \n",
       "76      194     15635.980532  10738.295483   8994.248712  6457.453908   \n",
       "77      195     19091.963238  10102.579353   9669.424490  5217.406785   \n",
       "78      198      8642.316668   8907.860860   7586.528884  6510.086878   \n",
       "\n",
       "             max  Diff Median with Optimizer  cycle_time_raw  \n",
       "0   15001.063149                 4284.962852        8805.244  \n",
       "1   18802.096051                -4498.401837        9431.527  \n",
       "2   12085.923660                 -499.754975       14622.587  \n",
       "3   16367.334062                 2043.850014        6757.300  \n",
       "4    9268.303847                -6596.997041       12222.587  \n",
       "..           ...                         ...             ...  \n",
       "74  18675.799977                -6922.594940       12139.685  \n",
       "75  18984.668347                -3523.065926        9667.500  \n",
       "76  19601.174884                -6641.731819       14403.291  \n",
       "77  19050.544967                -9422.538748        5496.187  \n",
       "78  14394.398955                -1055.787784        7550.780  \n",
       "\n",
       "[79 rows x 8 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
